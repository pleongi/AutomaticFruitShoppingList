{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.879 - TFM - Área 2 aula 1 - Machine Learning</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster universitario de Ciencia de datos (<i>Data Science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "# Adquisición de datos\n",
    "En este módulo se pretenden descargar las imágenes de ImageNet de las clases manzana, pera, plátano, limón, naranja y piña, para poder obtener un primer conjunto de imágenes de entrenamiento y de validación.\n",
    "Por cada clase descargaremos 200 de entrenamiento y 50 de validación. Asegurando que no existe ninguna imagen que estè repetida en la carpeta de entrenamiento y en la de validación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests\n",
    "import cv2\n",
    "import urllib\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 32, 32 #number of rows and columns to convert the images to\n",
    "input_shape = (img_rows, img_cols, 3)#format to store the images (rows, columns,channels) called channels last\n",
    "def url_to_image(url):\n",
    "    # download the image, convert it to a NumPy array, and then read it into OpenCV format\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    return image\n",
    "n_of_training_images=200#the number of training images to use\n",
    "n_of_validating_images=50#the number of validating images to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "synsetsIds=['n07739125','n07767847','n07753592','n07749582','n07747607','n07753275']\n",
    "synsetsNames=['n07739125','n07767847','n07753592','n07749582','n07747607','n07753275']\n",
    "#synsetsNames=['apple','pear','banana','lemon','orange','pineapple']\n",
    "\n",
    "for i in range(0,len(synsetsIds)):\n",
    "    url=\"http://www.image-net.org/api/text/imagenet.synset.geturls.getmapping?wnid=\"+synsetsIds[i]\n",
    "    page = requests.get(url)\n",
    "    train_path=\"TensorFlow/workspace/images/train/\"+synsetsNames[i]+\"/\"\n",
    "    validation_path=\"TensorFlow/workspace/images/test/\"+synsetsNames[i]+\"/\"\n",
    "    \n",
    "    shutil.rmtree(train_path, ignore_errors=True)\n",
    "    os.makedirs(train_path)\n",
    "    \n",
    "    shutil.rmtree(validation_path, ignore_errors=True)\n",
    "    os.makedirs(validation_path)\n",
    "        \n",
    "    #puts the content of the website into the soup variable, each url on a different line\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    str_soup=str(soup)#convert soup to string so it can be split\n",
    "    type(str_soup)\n",
    "    split_urls=str_soup.split('\\r\\n')#split so each url is a different possition on a list\n",
    "    #Train data:\n",
    "    print(\"Downloading training data: \"+synsetsNames[i])\n",
    "    progress=0\n",
    "    progress_tmp=0\n",
    "    while True:\n",
    "    #for progress in range(n_of_training_images):#store all the images on a directory\n",
    "        if split_urls[progress_tmp] != None:\n",
    "            try:\n",
    "                I = url_to_image(split_urls[progress_tmp].split(' ', 1)[1])\n",
    "                if (len(I.shape))==3: #check if the image has width, length and channels\n",
    "                    save_path = train_path+str(split_urls[progress_tmp].split(' ', 1)[0])+'.jpg'#create a name of each image\n",
    "                    cv2.imwrite(save_path,I)\n",
    "                    progress+=1\n",
    "                    #print (\"Progress:\",(progress/n_of_training_images)*100)\n",
    "            except:\n",
    "                None\n",
    "            progress_tmp+=1\n",
    "            if progress == n_of_training_images:\n",
    "                break\n",
    "                \n",
    "    #Testing data:\n",
    "    print(\"Downloading testing data: \"+synsetsNames[i])\n",
    "    progress2=0\n",
    "    progress_tmp2=0\n",
    "    while True:\n",
    "        #for progress in range(n_of_validating_images):#store all the images on a directory\n",
    "        if not split_urls[progress+progress2] == None:\n",
    "            try:\n",
    "                #get images that are different from the ones used for training\n",
    "                I = url_to_image(split_urls[progress_tmp+progress_tmp2].split(' ', 1)[1])\n",
    "                if (len(I.shape))==3: #check if the image has width, length and channels\n",
    "                    save_path = validation_path+str(split_urls[progress_tmp+progress_tmp2].split(' ', 1)[0])+'.jpg'#create a name of each image\n",
    "                    cv2.imwrite(save_path,I)\n",
    "                    progress2+=1\n",
    "                    #print (\"Progress:\",(progress2/n_of_validating_images)*100)\n",
    "            except:\n",
    "                None\n",
    "            progress_tmp2+=1\n",
    "            if progress2 == n_of_validating_images:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación:\n",
    "Validación de que no existen imágenes repetidas en train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking: n07739125\n",
      "Finished checking: n07767847\n",
      "Finished checking: n07753592\n",
      "Finished checking: n07749582\n",
      "Finished checking: n07747607\n",
      "Finished checking: n07753275\n",
      "Checked 300 validation images\n"
     ]
    }
   ],
   "source": [
    "synsetsIds=['n07739125','n07767847','n07753592','n07749582','n07747607','n07753275']\n",
    "synsetsNames=['n07739125','n07767847','n07753592','n07749582','n07747607','n07753275']\n",
    "#synsetsNames=['apple','pear','banana','lemon','orange','pineapple']\n",
    "\n",
    "counter=0\n",
    "for i in range(0,len(synsetsIds)):\n",
    "    train_path=\"TensorFlow/workspace/images/train/\"+synsetsNames[i]+\"/\"\n",
    "    validation_path=\"TensorFlow/workspace/images/test/\"+synsetsNames[i]+\"/\"\n",
    "    train_paths=os.listdir(train_path)\n",
    "    validation_paths=os.listdir(validation_path)\n",
    "    for file in validation_paths:\n",
    "        counter+=1\n",
    "        #print(\"Checking file:\", file)\n",
    "        if file in train_paths:\n",
    "            print(\"Error duplicated image [\"+str(file)+\"] in train and test for: \"+synsetsNames[i])\n",
    "    print(\"Finished checking: \"+synsetsNames[i])\n",
    "print(\"Checked \"+str(counter)+\" validation images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación de que no existe una máscara en png y un fichero xml con las bounding boxes por cada imagen en train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking: n07739125\n",
      "Finished checking: n07767847\n",
      "Finished checking: n07753592\n",
      "Finished checking: n07749582\n",
      "Finished checking: n07747607\n",
      "Finished checking: n07753275\n",
      "Checked 300 validation images\n"
     ]
    }
   ],
   "source": [
    "synsetsIds=['n07739125','n07767847','n07753592','n07749582','n07747607','n07753275']\n",
    "synsetsNames=['n07739125','n07767847','n07753592','n07749582','n07747607','n07753275']\n",
    "#synsetsNames=['apple','pear','banana','lemon','orange','pineapple']\n",
    "\n",
    "for i in range(0,len(synsetsIds)):\n",
    "    train_path=\"TensorFlow/workspace/images/train/\"+synsetsNames[i]+\"/\"\n",
    "    validation_path=\"TensorFlow/workspace/images/test/\"+synsetsNames[i]+\"/\"\n",
    "    xml_annotations_path=\"TensorFlow/workspace/annotations/bounding_boxes/\"+synsetsNames[i]+\"/Annotation/\"+synsetsNames[i]+\"/\"\n",
    "    masks_annotations_path=\"TensorFlow/workspace/annotations/masks/\"\n",
    "    \n",
    "    validation_paths=os.listdir(validation_path)\n",
    "    validation_pathsli=[x.split('.')[0] for x in validation_paths]\n",
    "    \n",
    "    train_paths=os.listdir(train_path)\n",
    "    train_pathsli=[x.split('.')[0] for x in train_paths]\n",
    "    \n",
    "    mask_paths=os.listdir(masks_annotations_path)\n",
    "    mask_pathsli=[x.split('.')[0] for x in mask_paths]\n",
    "    \n",
    "    xml_paths=os.listdir(xml_annotations_path)\n",
    "    xml_pathsli=[x.split('.')[0] for x in xml_paths]\n",
    "    \n",
    "    for file in train_pathsli:\n",
    "        if file+\"_color_mask\" not in mask_pathsli:\n",
    "            print(\"Error not mask for image [\"+str(file)+\"] in train for: \"+synsetsNames[i])\n",
    "        if file not in xml_pathsli:\n",
    "            print(\"Error not xml for image [\"+str(file)+\"] in train for: \"+synsetsNames[i])\n",
    "            \n",
    "    for file in validation_pathsli:\n",
    "        if file+\"_color_mask\" not in mask_pathsli:\n",
    "            print(\"Error not mask fo## Validación:\n",
    "Validación de que no existen imágenes repetidas en train y test.r image [\"+str(file)+\"] in test for: \"+synsetsNames[i])\n",
    "        if file not in xml_pathsli:\n",
    "            print(\"Error not xml for image [\"+str(file)+\"] in test for: \"+synsetsNames[i])\n",
    "            \n",
    "    print(\"Finished checking: \"+synsetsNames[i])\n",
    "print(\"Checked \"+str(counter)+\" validation images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
